{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6a9817dc",
      "metadata": {
        "id": "6a9817dc"
      },
      "source": [
        "\n",
        "# Hands-On Machine Learning: Core Topics\n",
        "\n",
        "This notebook mirrors the style of the attached hands-on template and walks you through **six core topics** in Machine Learning:\n",
        "\n",
        "1. Supervised Learning – Regression  \n",
        "2. Supervised Learning – Classification  \n",
        "3. Unsupervised Learning – Clustering  \n",
        "4. Dimensionality Reduction  \n",
        "5. Cross-Validation  \n",
        "6. Bias–Variance and Overfitting\n",
        "\n",
        "> **How to use this notebook**\n",
        "> - Run each cell in order (Shift+Enter).\n",
        "> - Read the short theory blocks, then experiment by changing parameters.\n",
        "> - Complete the **Try it / Exercises** prompts sprinkled throughout.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e6f22a6",
      "metadata": {
        "id": "0e6f22a6"
      },
      "source": [
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "\n",
        "- Train and evaluate simple **regression** and **classification** models.\n",
        "- Apply **K-Means** and **DBSCAN** for clustering and reason about their differences.\n",
        "- Use **PCA** to reduce dimensionality and visualize high-dimensional datasets.\n",
        "- Perform **K-fold cross-validation** and **grid search** for model selection.\n",
        "- Interpret **learning curves** and **validation curves** to diagnose **under/overfitting**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f976cf",
      "metadata": {
        "id": "68f976cf"
      },
      "source": [
        "\n",
        "## Setup\n",
        "\n",
        "We use common scientific Python libraries. If something is missing in your environment, install it via pip (uncomment the line below).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c5e4c0c1",
      "metadata": {
        "id": "c5e4c0c1"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf3260cd",
      "metadata": {
        "id": "bf3260cd"
      },
      "source": [
        "\n",
        "## 1. Supervised Learning – Regression\n",
        "\n",
        "\n",
        "### Goal:\n",
        "Predict a continuous target (e.g., house price).  \n",
        "We start with a synthetic dataset and compare **Linear Regression** and **Ridge Regression**.\n",
        "\n",
        "**Why Ridge?** Adds L2 penalty to shrink coefficients → often better test performance with noisy/correlated features.\n",
        "\n",
        "**Metrics:** **MSE** (↓ better), **R²** (↑ better; can be < 0 on test).\n",
        "\n",
        "### Steps\n",
        "- Generate data with `make_regression`.\n",
        "- Split with `train_test_split`.\n",
        "- Fit `LinearRegression()` and `Ridge(alpha=1.0)`.\n",
        "- Compute test **MSE** and **R²**.\n",
        "- Plot test scatter + both prediction lines.\n",
        "\n",
        "### Interpret\n",
        "- Ridge better test MSE/R² ⇒ less overfitting.\n",
        "- If Ridge underperforms, reduce `alpha`; too large `alpha` ⇒ underfit.\n",
        "\n",
        "### Try\n",
        "- Change `noise` in `make_regression`. How do MSE and R² respond?\n",
        "- Vary `alpha` in `Ridge(alpha=...)`. When does Ridge help vs hurt?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c1539f1f",
      "metadata": {
        "id": "c1539f1f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d6c3d6b",
      "metadata": {
        "id": "7d6c3d6b"
      },
      "source": [
        "\n",
        "## 2. Supervised Learning – Classification\n",
        "\n",
        "### Goal:\n",
        "Predict a discrete class (e.g., species).  \n",
        "We use a 2-class synthetic dataset to compare **Logistic Regression** and **SVM**.\n",
        "\n",
        "### Steps\n",
        "1. Generate a binary dataset with `make_classification` (tune `class_sep`).\n",
        "2. Do a `train_test_split`.\n",
        "3. Build two pipelines: `StandardScaler()` → `LogisticRegression()` and `SVC(kernel='rbf')`.\n",
        "4. Evaluate **accuracy**; plot **confusion matrices**; print `classification_report`.\n",
        "\n",
        "### Interpretation\n",
        "- **SVM (RBF)** often wins when the decision boundary is non-linear.  \n",
        "- **LogReg** is fast and interpretable for near-linear problems.  \n",
        "- Use confusion matrices to see **which** classes are confused.\n",
        "\n",
        "### Try / Experiments\n",
        "- Vary `class_sep` and `n_informative`.  \n",
        "- Change SVM kernel (`'linear'`, `'poly'`) and tune `C`, `gamma`.  \n",
        "- Use `load_iris()` (3 classes) nd adapt the code to multiclass classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ea3f9864",
      "metadata": {
        "id": "ea3f9864"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f71a475",
      "metadata": {
        "id": "7f71a475"
      },
      "source": [
        "\n",
        "## 3. Unsupervised Learning – Clustering\n",
        "\n",
        "### Goal:\n",
        "Group similar points without labels.  \n",
        "\n",
        "\n",
        "###  Algorithms\n",
        "\n",
        "- K-Means: centroid-based, needs k, struggles with non-spherical shapes/outliers.\n",
        "- DBSCAN: density-based, finds arbitrary shapes, flags noise (label = −1), no k.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. Generate blobs + add uniform noise.\n",
        "2. Fit KMeans(n_clusters=4) → labels_km.\n",
        "3. Fit DBSCAN(eps=0.9, min_samples=10) → labels_db.\n",
        "4. Scatter-plot points colored by labels for both methods.\n",
        "\n",
        "### Interpret\n",
        "\n",
        "- K-Means: clean spherical clusters; noisy points get forced into nearest cluster.\n",
        "- DBSCAN: can separate irregular clusters; noisy points → −1. Too small eps → over-fragmentation; too large → merges clusters.\n",
        "\n",
        "### Try\n",
        "\n",
        "- Vary cluster_std (data overlap).\n",
        "- Sweep n_clusters for K-Means; plot inertia (elbow).\n",
        "- Grid eps × min_samples for DBSCAN; count noise and clusters.\n",
        "- Standardize features if scales differ.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ca17a67e",
      "metadata": {
        "id": "ca17a67e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "224e5b22",
      "metadata": {
        "id": "224e5b22"
      },
      "source": [
        "\n",
        "## 4. Dimensionality Reduction\n",
        "\n",
        "### Goal:\n",
        "Compress features while preserving structure.  \n",
        "We use **PCA** to project to 2D for visualization.\n",
        "\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. Load data (iris), set X, y.\n",
        "2. Build Pipeline(StandardScaler() → PCA(n_components=2)).\n",
        "3. fit_transform(X) → X_pca.\n",
        "4. Scatter plot by class (PC1 vs PC2).\n",
        "5. Print explained_variance_ratio_ and its sum.\n",
        "\n",
        "### Interpret\n",
        "\n",
        "- Classes separating in PC space ⇒ PCA captures meaningful variance.\n",
        "- Explained variance (EVR): higher cumulative EVR ⇒ better 2D summary.\n",
        "- If clusters overlap, 2 PCs may be insufficient or classes aren’t linearly separable.\n",
        "\n",
        "### Try\n",
        "\n",
        "- Change n_components (e.g., 3) and compare cumulative EVR.\n",
        "- Remove scaling and observe impact.\n",
        "- Use whiten=True and compare plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "57eabe69",
      "metadata": {
        "id": "57eabe69"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3082f7b4",
      "metadata": {
        "id": "3082f7b4"
      },
      "source": [
        "\n",
        "## 5. Cross-Validation\n",
        "\n",
        "We estimate generalization performance by splitting the data into multiple folds.\n",
        "\n",
        "### Goal:\n",
        "Use **K-Fold** and **`cross_val_score`** on classification with Logistic Regression.\n",
        "\n",
        "\n",
        "### Method:\n",
        " Use K-Fold CV with a scaled Logistic Regression pipeline. Report fold scores and the mean. Then run GridSearchCV to pick the best C.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. Create data with make_classification.\n",
        "2. Build Pipeline(StandardScaler(), LogisticRegression(...)).\n",
        "3. Define KFold(n_splits=5, shuffle=True, random_state=...).\n",
        "4. cross_val_score(..., scoring=\"accuracy\") → fold scores + mean.\n",
        "5. GridSearchCV over clf__C (and solver/penalty), fit, read best_params_, best_score_.\n",
        "\n",
        "### Interpret\n",
        "\n",
        "- Stable, high mean + low std ⇒ robust model.\n",
        "- GridSearchCV selects C balancing bias/variance.\n",
        "\n",
        "### Try\n",
        "\n",
        "- Try `SVC` instead of `LogisticRegression` and grid-search `C` and `gamma`.\n",
        "- Change `n_splits` to 3 or 10. How does variance of scores change?\n",
        "- Use `scoring='roc_auc'` on a binary dataset and compare with accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dded0388",
      "metadata": {
        "id": "dded0388"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "141a3831",
      "metadata": {
        "id": "141a3831"
      },
      "source": [
        "\n",
        "## 6. Bias–Variance and Overfitting\n",
        "\n",
        "### Goal:\n",
        "Understand model capacity vs data size/complexity.\n",
        "\n",
        "We will:\n",
        "- Plot a **learning curve** (train size vs. score) to see if more data would help.\n",
        "- Plot a **validation curve** (hyperparameter vs. score) to see under/overfitting.\n",
        "\n",
        "\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. Build a DecisionTreeClassifier pipeline.\n",
        "2. Use learning_curve with train_sizes=np.linspace(0.1,1.0,6).\n",
        "3. Plot training vs CV accuracy.\n",
        "4. Use validation_curve over max_depth=1..20.\n",
        "5. Plot train vs CV accuracy vs max_depth.\n",
        "\n",
        "### Interpret\n",
        "\n",
        "- High train, low CV ⇒ overfitting (reduce complexity / more data).\n",
        "- Low train and CV ⇒ underfitting (increase complexity / features).\n",
        "- Learning curve flat & gap large ⇒ more data may help; if both low, increase model capacity.\n",
        "\n",
        "### Try\n",
        "\n",
        "- Repeat with `DecisionTreeClassifier(min_samples_leaf=...)`. How does it affect overfitting?\n",
        "- Swap in `Ridge` (regression) and use `validation_curve` over `alpha`.\n",
        "- Change dataset size and noise; re-plot learning curves to see effects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a03d4885",
      "metadata": {
        "id": "a03d4885"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "234d5994",
      "metadata": {
        "id": "234d5994"
      },
      "source": [
        "\n",
        "## Wrap-up\n",
        "\n",
        "In this hands-on you practiced:\n",
        "- Building and evaluating **regression** and **classification** models\n",
        "- Applying **K-Means** and **DBSCAN** for clustering\n",
        "- Using **PCA** to reduce dimensions and visualize data\n",
        "- Doing **K-fold cross-validation** and **grid search**\n",
        "- Reading **learning** and **validation curves** to reason about **bias–variance**\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "title": "Hands-On Machine Learning: Core Topics"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}