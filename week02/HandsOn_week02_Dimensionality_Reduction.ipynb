{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "da68cc3d",
      "metadata": {
        "id": "da68cc3d"
      },
      "source": [
        "\n",
        "# Dimensionality Reduction — Hands‑On\n",
        "\n",
        "**Course:** Data Analysis & Machine Learning for Physics  \n",
        "**Focus:** PCA, SVD, t‑SNE, UMAP, Autoencoders (same dataset for all methods)  \n",
        "**Dataset:** `sklearn.datasets.load_digits()` (1797 samples, 8×8 images → 64D feature vectors)\n",
        "\n",
        "**What you'll do in ~60 minutes**\n",
        "- Load and standardize the dataset (digits) — quick, reproducible, small but rich.\n",
        "- Apply **PCA**: visualize 2‑D embedding, **explained variance ratio**, **cumulative variance**.\n",
        "- Apply **SVD**: inspect **singular values (log‑scale)** and **cumulative energy**; connect to PCA.\n",
        "- Apply **t‑SNE** and **UMAP**: study the effect of **hyperparameters** on neighborhood structure.\n",
        "- Build a tiny **Autoencoder** with a 2‑D latent space and compare to PCA/t‑SNE/UMAP embeddings.\n",
        "\n",
        "> This lab complements the slide deck *“Dimensionality Reduction: PCA, SVD, t‑SNE, UMAP, and Autoencoders.”*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba432382",
      "metadata": {
        "id": "ba432382"
      },
      "source": [
        "\n",
        "## 0) Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b832a1fa",
      "metadata": {
        "id": "b832a1fa"
      },
      "source": [
        "\n",
        "## 1) Load & Standardize the Dataset\n",
        "\n",
        "We will use `sklearn.datasets.load_digits()`. It provides 1797 images of handwritten digits, each of size 8×8, flattened into 64‑dim vectors: digits = load_digits()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b58339d",
      "metadata": {
        "id": "0b58339d"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 2) Principal Component Analysis (PCA)\n",
        "\n",
        "**Exercise 2.1 .**  \n",
        "Fit PCA on `X_std` and:\n",
        "1. Produce a **2‑D scatter** of the first two principal components (PC1 vs PC2) colored by digit label.\n",
        "2. Plot the **scree plot** of explained variance ratio and the **cumulative explained variance**.  \n",
        "3. Report the **minimum number of components** to explain at least **95%** of the variance.\n",
        "\n",
        "<details><summary><strong>Why this matters (physics intuition)</strong></summary>\n",
        "PCA diagonalizes the covariance matrix — analogous to finding **eigenmodes** (normal modes) of a system. The eigenvalues represent variances captured by each mode.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "631e5568",
      "metadata": {
        "id": "631e5568"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 3) Singular Value Decomposition (SVD)\n",
        "\n",
        "**Exercise 3.1 .**  \n",
        "Compute the compact SVD of the **centered** data `X_centered = X_std` (already zero-mean) using `np.linalg.svd`:\n",
        "1. Plot the **singular values** \\(\\sigma_i\\) on a **log scale**.\n",
        "2. Plot the **cumulative energy** defined as  \n",
        "\\[ E(k) = \\frac{\\sum_{i=1}^k \\sigma_i^2}{\\sum_{i=1}^r \\sigma_i^2} \\]  \n",
        "and report the **k** for which **E(k) ≥ 0.95**.  \n",
        "3. Verify: PCA eigenvalues are proportional to \\(\\sigma_i^2/(n-1)\\).\n",
        "\n",
        "<details><summary><strong>Connection to PCA</strong></summary>\n",
        "For centered data, PCA directions equal right singular vectors of **X**, and PCA variances equal \\(\\sigma_i^2/(n-1)\\). Many libraries implement PCA via SVD for numerical stability.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "394d0229",
      "metadata": {
        "id": "394d0229"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 4) t‑SNE (t‑distributed Stochastic Neighbor Embedding)\n",
        "\n",
        "**Exercise 4.1 .**  \n",
        "Run t‑SNE on `X_std` with different **perplexities** and compare the 2‑D embeddings:\n",
        "\n",
        "- Perplexities: `[5, 30, 50]` (keep other params default except `random_state=42`)\n",
        "- For each run, make a scatter plot colored by labels and briefly note cluster compactness and separation.\n",
        "\n",
        "> **Tip:** t‑SNE focuses on *local* neighborhood preservation and is sensitive to hyperparameters. Expect different layouts across runs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27619504",
      "metadata": {
        "id": "27619504"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 5) UMAP (Uniform Manifold Approximation and Projection)\n",
        "\n",
        "**Exercise 5.1 .**  \n",
        "If available, run UMAP with different **n_neighbors** and **min_dist**, and compare the embeddings:\n",
        "- Try pairs `(n_neighbors, min_dist)` in `[(5, 0.1), (15, 0.1), (50, 0.5)]`.\n",
        "- Discuss local cluster separation vs global structure preservation.\n",
        "\n",
        "> **Note:** UMAP is often faster and gives more stable global geometry than t‑SNE, but hyperparameters still matter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cc90089",
      "metadata": {
        "id": "1cc90089"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 6) Autoencoder (Neural Dimensionality Reduction)\n",
        "\n",
        "We will build a tiny fully connected autoencoder with a **2‑D latent space**.\n",
        "\n",
        "**Exercise 6.1 .**\n",
        "1. Define an encoder–decoder with architecture 64→32→**2** (latent)→32→64 using ReLU except for the output (sigmoid).\n",
        "2. Train it to reconstruct standardized inputs (`X_std`).\n",
        "3. Extract the 2‑D latent embedding and plot it colored by labels.\n",
        "4. Compare **reconstruction MSE** vs a **2‑D PCA** reconstruction.\n",
        "\n",
        "> If `tensorflow` is not available, install it or skip to the comparative discussion below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "278e04b1",
      "metadata": {
        "id": "278e04b1"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 7) Comparative Discussion \n",
        "\n",
        "**Pros/Cons quick matrix (for this dataset):**\n",
        "- **PCA** — fast, interpretable; linear; good for variance capture and denoising.\n",
        "- **SVD** — general factorization; connects cleanly to PCA; singular values show energy spectrum.\n",
        "- **t‑SNE** — excellent local cluster separation; stochastic layouts; sensitive to perplexity.\n",
        "- **UMAP** — often faster, balances local & global; sensitive to `n_neighbors`/`min_dist`.\n",
        "- **Autoencoders** — flexible nonlinear reduction; needs training; architecture & optimization matter.\n",
        "\n",
        "**Prompts:**\n",
        "- Does your PCA scree plot show a “knee”? Where would you truncate?\n",
        "- Which method best separates classes? Which preserves global distances better?\n",
        "- How do t‑SNE/UMAP hyperparameters affect cluster compactness vs. continuity?\n",
        "- Compare reconstruction errors: AE(2D) vs PCA(2D). What might improve the AE (depth, nonlinearities, epochs)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea301c0",
      "metadata": {
        "id": "3ea301c0"
      },
      "source": [
        "\n",
        "### (Optional) Visualize Reconstructions\n",
        "\n",
        "Pick a few samples and compare original vs. reconstructions from PCA‑k and Autoencoder.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
