{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81472ef1",
   "metadata": {},
   "source": [
    "\n",
    "# üß™ Autonomous Activity: Dimensionality Reduction on Embryo Development Timelapse\n",
    "\n",
    "In this hands-on activity, you will apply a variety of dimensionality reduction techniques to analyze real microscopy data. The dataset contains time-lapse images of normal and mutant embryos. Each image stack has approximately 450 frames.\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Objectives:**\n",
    "- Explore, visualize, and preprocess multi-frame `.tif` images.\n",
    "- Test and compare different data normalization strategies (e.g., [0,1] scaling vs StandardScaler).\n",
    "- Use PCA, SVD, t-SNE, UMAP, and Autoencoders to extract and visualize developmental trajectories.\n",
    "- Identify biological differences in developmental dynamics between embryo types.\n",
    "- Reflect on the advantages, limitations, and behaviors of each technique.\n",
    "\n",
    "üìÅ **Dataset:** [Google Drive Link](https://drive.google.com/drive/folders/1_qxqm-v5yCrme3pAW2rjyOOXIeQDuV54?usp=drive_link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da91bfca",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load and Explore the Dataset\n",
    "\n",
    "Each `.tif` file contains ~450 grayscale frames. Your first task is to:\n",
    "- Load the 3 `.tif` files using `tifffile.imread`.\n",
    "- Normalize each image stack using two strategies:\n",
    "  - [0, 1] Min-Max normalization\n",
    "  - Standardization using `StandardScaler`\n",
    "- Plot a few representative frames across time for each embryo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbea306",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tifffile import imread\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_minmax(img):\n",
    "    return (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "def normalize_standard(img):\n",
    "    flat = img.reshape(img.shape[0], -1)\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(flat).reshape(img.shape)\n",
    "\n",
    "def show_frames(stack, title, step=100):\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    for i in range(5):\n",
    "        frame = stack[i * step]\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(frame, cmap=\"gray\")\n",
    "        plt.title(f\"{title} t={i*step}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Replace this path with the local folder where your .tif files are stored\n",
    "folder_path = \"path_to_downloaded_tif_folder\"\n",
    "file_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".tif\")]\n",
    "stacks = [imread(fp) for fp in file_paths]\n",
    "names = [os.path.basename(fp).split(\".\")[0] for fp in file_paths]\n",
    "\n",
    "# Show examples (min-max normalized)\n",
    "for name, stack in zip(names, stacks):\n",
    "    show_frames(normalize_minmax(stack), title=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e6ddb",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Preprocess the Data\n",
    "\n",
    "Flatten each frame and construct a matrix `X` with shape `(n_frames, n_pixels)` for each normalization method. Then create a label vector:\n",
    "\n",
    "- `label = 0` ‚Üí control embryo\n",
    "- `label = 1` ‚Üí mutant 1\n",
    "- `label = 2` ‚Üí mutant 2\n",
    "\n",
    "This step will prepare the input data for dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_minmax, X_standard, y = [], [], []\n",
    "\n",
    "for i, stack in enumerate(stacks):\n",
    "    norm_minmax = normalize_minmax(stack)\n",
    "    norm_standard = normalize_standard(stack)\n",
    "    X_minmax.append(norm_minmax.reshape(stack.shape[0], -1))\n",
    "    X_standard.append(norm_standard.reshape(stack.shape[0], -1))\n",
    "    y.append(np.full(stack.shape[0], i))\n",
    "\n",
    "X_minmax = np.vstack(X_minmax)\n",
    "X_standard = np.vstack(X_standard)\n",
    "y = np.concatenate(y)\n",
    "\n",
    "print(\"MinMax shape:\", X_minmax.shape)\n",
    "print(\"StandardScaler shape:\", X_standard.shape)\n",
    "print(\"Labels:\", np.unique(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21673bc",
   "metadata": {},
   "source": [
    "\n",
    "> üß™ **Experiment**: Try running all dimensionality reduction methods with both versions of the input (`X_minmax` and `X_standard`) and compare how they affect embeddings and separability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb087831",
   "metadata": {},
   "source": [
    "\n",
    "## 3. PCA (Principal Component Analysis)\n",
    "\n",
    "Apply PCA on both normalized datasets:\n",
    "- Visualize the 2D PCA embedding colored by embryo type.\n",
    "- Compare the separation between classes for `X_minmax` and `X_standard`.\n",
    "- Plot the **explained variance ratio** and **cumulative variance** for each.\n",
    "\n",
    "> üß† Tip: Use `PCA(n_components=2)` for plotting and `PCA(n_components=50)` to analyze cumulative variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb0ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def run_pca(X, labels, title=\"\"):\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    plt.figure()\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap=\"tab10\", s=5)\n",
    "    plt.colorbar(scatter, label=\"Embryo type\")\n",
    "    plt.title(f\"PCA Embedding - {title}\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()\n",
    "\n",
    "    # Scree plot\n",
    "    pca_full = PCA().fit(X)\n",
    "    cum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    plt.figure()\n",
    "    plt.plot(cum_var, marker=\"o\")\n",
    "    plt.axhline(0.95, linestyle=\"--\", color=\"r\")\n",
    "    plt.title(f\"Cumulative Explained Variance - {title}\")\n",
    "    plt.xlabel(\"Number of components\")\n",
    "    plt.ylabel(\"Cumulative variance\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# PCA comparison\n",
    "run_pca(X_minmax, y, \"Min-Max Normalization\")\n",
    "run_pca(X_standard, y, \"StandardScaler\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223d97b6",
   "metadata": {},
   "source": [
    "\n",
    "## 4. SVD (Singular Value Decomposition)\n",
    "\n",
    "Apply SVD to both datasets and analyze:\n",
    "- The decay of singular values on a log scale.\n",
    "- The cumulative energy of singular values.\n",
    "- Compare how quickly each normalization captures energy.\n",
    "\n",
    "> üîç Insight: SVD reveals the inherent structure of your dataset. A sharper drop often suggests stronger linear compressibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45434fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_svd(X, title=\"\"):\n",
    "    U, S, Vt = np.linalg.svd(X - X.mean(axis=0), full_matrices=False)\n",
    "    energy = np.cumsum(S**2) / np.sum(S**2)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(np.log10(S), marker=\"o\")\n",
    "    plt.title(f\"SVD - Singular Values (log10) - {title}\")\n",
    "    plt.xlabel(\"Component\")\n",
    "    plt.ylabel(\"log10(œÉ)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(energy, marker=\"o\")\n",
    "    plt.axhline(0.95, linestyle=\"--\", color=\"r\")\n",
    "    plt.title(f\"SVD - Cumulative Energy - {title}\")\n",
    "    plt.xlabel(\"Component\")\n",
    "    plt.ylabel(\"Cumulative energy\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_svd(X_minmax, \"Min-Max\")\n",
    "plot_svd(X_standard, \"StandardScaler\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9e357b",
   "metadata": {},
   "source": [
    "\n",
    "## 5. t-SNE\n",
    "\n",
    "Use t-SNE to capture local structure and dynamics:\n",
    "- Run t-SNE with different perplexities `[5, 30, 100]`.\n",
    "- Plot the 2D embeddings and analyze how clusters behave.\n",
    "- Compare results between both normalized inputs.\n",
    "\n",
    "> ‚è≥ Note: t-SNE is computationally intensive and sensitive to hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e09416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def run_tsne(X, labels, title=\"\"):\n",
    "    for perp in [5, 30, 100]:\n",
    "        tsne = TSNE(n_components=2, perplexity=perp, init=\"pca\", random_state=42)\n",
    "        X_tsne = tsne.fit_transform(X)\n",
    "        plt.figure()\n",
    "        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap=\"tab10\", s=5)\n",
    "        plt.title(f\"t-SNE ({title}) - perplexity={perp}\")\n",
    "        plt.colorbar(scatter, label=\"Embryo\")\n",
    "        plt.show()\n",
    "\n",
    "run_tsne(X_minmax, y, \"Min-Max\")\n",
    "run_tsne(X_standard, y, \"StandardScaler\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2746164",
   "metadata": {},
   "source": [
    "\n",
    "## 6. UMAP\n",
    "\n",
    "Use UMAP for global structure visualization:\n",
    "- Try combinations of `n_neighbors` and `min_dist`.\n",
    "- Compare embeddings across normalization methods.\n",
    "- Observe both local clustering and trajectory smoothness.\n",
    "\n",
    "> üìå UMAP is faster and often preserves better continuity in developmental trajectories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6409a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import umap\n",
    "\n",
    "    def run_umap(X, labels, title=\"\"):\n",
    "        settings = [(5, 0.1), (15, 0.1), (50, 0.5)]\n",
    "        for n_n, min_d in settings:\n",
    "            reducer = umap.UMAP(n_neighbors=n_n, min_dist=min_d, random_state=42)\n",
    "            X_umap = reducer.fit_transform(X)\n",
    "            plt.figure()\n",
    "            scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels, s=5, cmap=\"tab10\")\n",
    "            plt.title(f\"UMAP ({title}) - n_neighbors={n_n}, min_dist={min_d}\")\n",
    "            plt.colorbar(scatter, label=\"Embryo\")\n",
    "            plt.show()\n",
    "\n",
    "    run_umap(X_minmax, y, \"Min-Max\")\n",
    "    run_umap(X_standard, y, \"StandardScaler\")\n",
    "except ImportError:\n",
    "    print(\"UMAP not installed. Run: pip install umap-learn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e908d77b",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Autoencoder\n",
    "\n",
    "Train a neural autoencoder to learn a 2D latent space:\n",
    "- Architecture: `input ‚Üí 128 ‚Üí 32 ‚Üí 2 ‚Üí 32 ‚Üí 128 ‚Üí output`\n",
    "- Use ReLU activations and MSE loss.\n",
    "- Plot the latent 2D representations colored by embryo.\n",
    "\n",
    "> üí° Autoencoders are flexible nonlinear methods that may capture dynamics not seen by linear projections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191feb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "\n",
    "    def run_autoencoder(X, labels, title=\"\"):\n",
    "        input_dim = X.shape[1]\n",
    "        encoder = models.Sequential([\n",
    "            layers.Input(shape=(input_dim,)),\n",
    "            layers.Dense(128, activation=\"relu\"),\n",
    "            layers.Dense(32, activation=\"relu\"),\n",
    "            layers.Dense(2)\n",
    "        ])\n",
    "        decoder = models.Sequential([\n",
    "            layers.Input(shape=(2,)),\n",
    "            layers.Dense(32, activation=\"relu\"),\n",
    "            layers.Dense(128, activation=\"relu\"),\n",
    "            layers.Dense(input_dim)\n",
    "        ])\n",
    "\n",
    "        autoencoder = models.Sequential([encoder, decoder])\n",
    "        autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "        autoencoder.fit(X, X, epochs=20, batch_size=64, verbose=0)\n",
    "\n",
    "        X_latent = encoder.predict(X)\n",
    "        plt.figure()\n",
    "        scatter = plt.scatter(X_latent[:, 0], X_latent[:, 1], c=labels, s=5, cmap=\"tab10\")\n",
    "        plt.title(f\"Autoencoder Latent Space - {title}\")\n",
    "        plt.colorbar(scatter, label=\"Embryo\")\n",
    "        plt.show()\n",
    "\n",
    "    run_autoencoder(X_minmax, y, \"Min-Max\")\n",
    "    run_autoencoder(X_standard, y, \"StandardScaler\")\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not installed. Run: pip install tensorflow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17faa7b7",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Final Reflection\n",
    "\n",
    "Write a short report answering the following:\n",
    "\n",
    "1. What differences in developmental dynamics did you observe?\n",
    "2. Which method best captured biologically relevant features?\n",
    "3. Which normalization (Min-Max or StandardScaler) led to better embeddings?\n",
    "4. At which point do mutant trajectories diverge from normal?\n",
    "5. How consistent were the results across methods (PCA, t-SNE, UMAP, Autoencoders)?\n",
    "\n",
    "> üìù Submit this as a 1-page summary or short presentation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
